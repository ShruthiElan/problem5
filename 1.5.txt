1.HADOOP IN LAYMAN TERMS
 Hadoop is a free, Java-based programming framework that supports the processing of large data sets in a distributed    computing environment.
 It is part of the Apache project sponsored by the Apache Software Foundation.
 where this data comes from and why is this all so important suddenly? Well, some of the data has always been there -  weather information, scientific results, human genomic data, federal data, credit-card information, shopping  transactions, and so on. And with the social media revolution, a lot more is generated today by users such as you and me.  All our photos, tweets, blog posts, comments need to be persisted and almost instantaneously processed. With millions of  users, that translates to several petabytes of data. For storing and processing these data sets, companies had to build  new data platforms. 
 “Hadoop” is the name of the toy elephant, belonging to the daughter of Doug Cutting – the Creator of Hadoop. So Doug  decided to name his software project after his daughters toy “Hadoop”. 
2.COMPONENTS OF HADOOP FRAMEWORK
1) Hadoop Common-

Apache Foundation has pre-defined set of utilities and libraries that can be used by other modules within the Hadoop ecosystem. For example, if HBase and Hive want to access HDFS they need to make of Java archives (JAR files) that are stored in Hadoop Common.

2) Hadoop Distributed File System (HDFS) -

The default big data storage layer for Apache Hadoop is HDFS.
 HDFS component creates several replicas of the data block to be distributed across different clusters for reliable and quick data access. HDFS comprises of 3 important components-NameNode, DataNode and Secondary NameNode.

3) MapReduce
MapReduce is a Java-based system created by Google where the actual data from the HDFS store gets processed efficiently. MapReduce breaks down a big data processing job into smaller tasks. MapReduce is responsible for the analysing large datasets in parallel before reducing it to find the results. In the Hadoop ecosystem, Hadoop MapReduce is a framework based on YARN architecture. 

4)YARN

YARN forms an integral part of Hadoop 2.0.YARN is great enabler for dynamic resource utilization on Hadoop framework as users can run various Hadoop applications without having to bother about increasing workloads.
Key Benefits of Hadoop 2.0 YARN Component-

It offers improved cluster utilization
Highly scalable
Beyond Java
Novel programming models and services
Agility

3.REASONS TO LEARN BIGDATA TECHNOLOGY
1. No signs of slowing down. What’s fueled the meteoric rise of larger data sets? More people have access to mobile devices that sense and acquire information through the likes of cameras, microphones, etc. Consider this, since 2012, about 2.5 exabytes of data are created daily! That said, the Big Data revolution will continue to grow.
And with Big Data the tools will evolve: Big Data: Spark or Hadoop?
2. Everyone uses Big Data. Think Big Data is limited to IT circles? Think again. Big Data is everywhere, from politics to health care to even sports. Big Data analysis played a crucial role in President Obama’s successful reelection in 2012. Healthcare organizations use it to provide more personalized prescriptions, predictive analysis, and many other services. And sports-wise, more teams are using Big Data analysis to scout for athletes who best fit their needs.
3. Information Managers in Demand. Someone has to be able to implement, run, and manage the software used to analyze Big Data. So, in conjunction with the rise of Big Data, the demand for information management specialists has increased. Knowing how to use Big Data technology/software can make you highly desirable in a number of industries, even more so if you are able to break down that data and make it more streamlined. Other desired Big Data skills include data mining, information warehousing, and ETL (Extract. Transform. Load).
4. Software Options Galore. Just how big is Big Data? Major players such as Microsoft, IBM, Oracle have spent billions investing in data analytics and management software. There are a lot of solutions out there. There are also open source options such as Apache Hadoop, meaning the cost of entry doesn’t have to be outrageous.
5. You’ll learn other tech. Want to help companies really leverage Big Data? You’ll need to be able to utilize cloud-based services. They not only can provide the storage needed for large amounts of data, but the power needed to manage and analyze all of the data. Also, because Big Data is more than just numbers (it also involves media files such as audio and video) your data analysis skills are likely to expand!


https://github.com/ShruthiElan/assignment1problem5.git
